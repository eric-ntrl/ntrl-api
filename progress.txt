# NTRL Article Neutralization - Progress Log

## Codebase Patterns
<!-- Add reusable patterns discovered during implementation here -->

---

## Iteration Log

### Setup (2025-01-12)
- Created PRD: docs/prd/article-neutralization-v1.md
- Created prd.json with 18 user stories across 5 phases
- Stored canon: docs/canon/neutralization-canon-v1.md
- Stored content spec: docs/canon/content-spec-v1.md
- Integrated deterministic grader: app/services/grader.py
- Grader spec: app/data/grader_spec_v1.json

### Story 1.1 - Integrate deterministic grader (COMPLETE)
- Verified grader.py exists at app/services/grader.py
- Verified grader_spec_v1.json exists at app/data/grader_spec_v1.json with all deterministic canon rules
- Confirmed `from app.services.grader import grade` works
- Confirmed grade() returns {overall_pass: bool, results: list}
- Note: Some canon rules (A2, A6, B4, B5, C1, C3, C4, D4) require semantic/LLM analysis - these are intentionally not in the deterministic grader

### Story 1.2 - Create /v1/grade endpoint (COMPLETE)
- Created app/schemas/grading.py with GradeRequest, GradeResponse, RuleResult schemas
- Added POST /v1/grade endpoint to app/routers/admin.py
- Endpoint accepts original_text, neutral_text, original_headline, neutral_headline
- Returns {overall_pass, results} from deterministic grader
- FastAPI returns 422 validation error for missing required fields (standard behavior)

### Story 1.3 - Create LLM quality scorer service (COMPLETE)
- Created app/services/quality_scorer.py with score_quality() function
- Returns QualityScore dataclass with {score: float, feedback: str, rule_violations: list}
- Embedded CANON_RUBRIC contains all A1-D4 rules as grading criteria
- Works with OpenAI (gpt-4o-mini) and Anthropic (claude-3-5-haiku) providers
- Tested OpenAI successfully; Anthropic code is correct but account has no credits

### Story 1.4 - Create 10-article test corpus (COMPLETE)
- Created tests/fixtures/test_corpus/ directory
- Created 10 JSON article files with: original_title, original_body, source_url, section, length, has_manipulative_language
- Sections covered: world (3), us (3), technology (2), business (2) - 4 unique sections
- Lengths: short (3), medium (4), long (3)
- 4 articles with manipulative language: 001 (urgency/clickbait), 003 (clickbait/selling), 005 (emotional), 010 (agenda signaling)
- Note: 4 pre-existing test failures unrelated to corpus (classifier, deduper, neutralizer auth)

### Story 1.5 - Schema migration for new output fields (COMPLETE)
- Migration file created: migrations/versions/005_article_neutralization_v1_schema.py
- StoryNeutralized model updated with: feed_title, feed_summary, detail_title, detail_brief, detail_full
- Old fields (neutral_headline, neutral_summary) migrated via column rename
- Structured fields removed: what_happened, why_it_matters, what_is_known, what_is_uncertain
- Migration runs successfully and is reversible

### Story 2.1 - Create shared system prompt with canon rules (COMPLETE)
- Created DEFAULT_ARTICLE_SYSTEM_PROMPT containing all A1-D4 canon rules
- Contains manipulation patterns from existing neutralizer
- References content spec constraints (feed_title, feed_summary, detail_* specs)
- Retrievable via get_article_system_prompt() -> get_prompt('article_system_prompt')
- Stored in database via existing prompt infrastructure

### Story 2.2 - Create filter prompt for Detail Full (COMPLETE)
- Created DEFAULT_FILTER_DETAIL_FULL_PROMPT user prompt template
- Instructs preservation of structure, quotes, factual detail
- Instructs removal of manipulation, urgency, editorial framing, publisher cruft
- Requests JSON output with filtered_article and spans array
- Spans include: field, start_char, end_char, original_text, action, reason
- Added get_filter_detail_full_prompt() and build_filter_detail_full_prompt() functions

### Story 2.3 - Implement Call 1: Filter & Track (COMPLETE)
- Added DetailFullResult dataclass to hold detail_full text and spans
- Added abstract _neutralize_detail_full(body: str) method to NeutralizerProvider base class
- Implemented in MockNeutralizerProvider using existing pattern-matching logic
- Implemented in OpenAINeutralizerProvider using article_system_prompt + filter_detail_full_prompt
- Implemented in GeminiNeutralizerProvider with same prompts
- Implemented in AnthropicNeutralizerProvider with same prompts (max_tokens=4096 for longer articles)
- Added helper functions _parse_span_action(), _parse_span_reason(), parse_detail_full_response()
- Added 8 tests for _neutralize_detail_full() covering return types, empty body, clean/manipulative content, span validity, determinism
- All 18 neutralizer tests pass

### Story 2.4 - Grade and iterate Detail Full until quality >= 8.5 (COMPLETE)
- Ran Detail Full generation on all 10 test corpus articles using OpenAI (gpt-4o-mini)
- **Deterministic grader: 10/10 passed** (all articles pass)
- **Average LLM quality score: 9.00** (target: 8.5)
- Individual scores: [9.0, 10.0, 7.0, 10.0, 8.0, 10.0, 9.0, 10.0, 10.0, 7.0]
- Lowest scores on articles 003 (7.0 - clickbait), 010 (7.0 - agenda signaling)
- Updated grader_spec_v1.json B3_NO_AGENDA_SIGNALING to include: "radical left", "radical right", "open borders"
- No prompt iteration required - quality threshold met on first run
- Results logged to scripts/detail_full_results.json

**Quality Analysis:**
- Articles without manipulative language (002, 004, 006, 007, 008, 009): Perfect/near-perfect scores (9.0-10.0)
- Articles with manipulative language (001, 003, 005, 010): Scores 7.0-9.0
- Filter successfully removes urgency (BREAKING, shocking), emotional amplifiers, and most agenda signaling
- Minor residual issues: some implied judgment in context-dependent phrases

**Learnings:**
- The filter prompt (DEFAULT_FILTER_DETAIL_FULL_PROMPT) is aggressive enough to remove most manipulation
- Quote preservation working correctly - emotional language inside attributed quotes preserved
- Non-manipulative articles correctly returned unchanged (0 spans)

### Story 3.1 - Create synthesis prompt for Detail Brief (COMPLETE)
- Created DEFAULT_SYNTHESIS_DETAIL_BRIEF_PROMPT user prompt template
- Specifies 3-5 short paragraphs, plain prose only (no headers, no bullets)
- Specifies implicit structure flow: grounding → context → state of knowledge → uncertainty
- Specifies quote rules: short, embedded, immediately attributed, non-emotional
- Requests plain text output (not JSON)
- Added get_synthesis_detail_brief_prompt() and build_synthesis_detail_brief_prompt() functions

**Key Prompt Features:**
- Word count guidance: 150-300 words typical
- Explicit list of what NOT to include: headers, bullets, dividers, calls to action, meta-commentary
- Examples of good vs bad quote usage
- Preserves facts, epistemic markers, attribution, and real conflict

### Story 3.2 - Implement Call 2: Synthesize (COMPLETE)
- Added abstract _neutralize_detail_brief(body: str) -> str method to NeutralizerProvider base class
- Implemented in MockNeutralizerProvider using sentence extraction + neutralization
  - Extracts sentences, filters short ones, groups into 3 paragraphs
  - Returns paragraphs joined by double newlines
- Implemented in OpenAINeutralizerProvider:
  - Uses get_article_system_prompt() + build_synthesis_detail_brief_prompt()
  - No JSON response format (returns plain text)
  - Temperature 0.3 for consistency
- Implemented in GeminiNeutralizerProvider:
  - Same prompt combination
  - No response_mime_type set (plain text output)
- Implemented in AnthropicNeutralizerProvider:
  - Same prompt combination
  - max_tokens=2048 (sufficient for 3-5 paragraph brief)
- All providers fall back to MockNeutralizerProvider when API key missing

**Pattern:**
All 3 LLM provider implementations follow the same structure:
1. Early return for empty body
2. Check API key, fall back to mock if missing
3. Get system prompt + build user prompt
4. Call LLM API with plain text response (no JSON mode)
5. Return stripped response text
6. Fall back to mock on exception

### Story 3.3 - Grade and iterate Detail Brief until quality >= 8.5 (COMPLETE)
- Ran Detail Brief generation on all 10 test corpus articles using OpenAI (gpt-4o-mini)
- Created test script: scripts/test_detail_brief.py (mirrors test_detail_full.py structure)
- **Deterministic grader: 10/10 passed** (achieved in multiple runs)
- **Average LLM quality score: 8.70** (achieved in multiple runs, target: 8.5)
- Results logged to scripts/detail_brief_results.json

**Key Iterations:**
1. Initial run: 7.20 avg, 8/10 grader (too many A1 violations - adding facts)
2. Added "CRITICAL: NO NEW FACTS" section with explicit prohibitions
3. Added "CRITICAL: MEANING PRESERVATION" section for scope/certainty markers
4. Fixed grader's scope_marker_preservation to use word boundaries (avoid "all" matching "eventually")
5. Added "PRIME DIRECTIVE" - "You are a FILTER, not a writer"
6. Final run: 8.70 avg, 10/10 grader

**Grader Improvements:**
- Fixed _scope_marker_preservation() to use word boundary regex: r'\b' + marker + r'\b'
- Fixed _certainty_marker_preservation() to use word boundary regex
- Prevents false positives like "all" matching inside "eventually" or "called"

**Prompt Improvements (DEFAULT_SYNTHESIS_DETAIL_BRIEF_PROMPT):**
- Added PRIME DIRECTIVE section: "You are a FILTER, not a writer"
- Added explicit scope marker verification: "If 'entire' appears in source, 'entire' MUST appear in output"
- Added EXPLICIT PROHIBITIONS section with banned phrases:
  - "ongoing efforts", "sustainability", "conservation", "management" (unless in source)
  - "remain to be seen", "may occur", "could occur" (unless in source)
  - "challenges", "concerns", "issues" as added framing
- Added "Brief should be SMALLER than the original, not larger"
- Added "Be SHORTER than the original (condense, don't expand)"

**Quality Analysis:**
- Articles without manipulative language (002, 004, 006, 007, 008, 009): Consistently 9.0
- Articles with manipulative language (001, 003, 005, 010): 7.0-9.0
- Main issues on lower-scoring articles: residual emotional language, scope marker preservation
- Article 006 (short article) improved from 5.0 to 9.0 by preventing LLM from adding context

**Learnings:**
- LLMs naturally want to add context/background - must explicitly prohibit this behavior
- Word boundary matching critical for deterministic grader accuracy
- Short articles need explicit "don't expand" guidance
- Scope markers (all, entire, every, multiple) easily dropped - require explicit verification instruction
- LLM quality scoring has ~0.5 point variance between runs
- Best to demonstrate criteria can be met across multiple runs (stochastic variance)
